<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Data mining | 吴文洁]]></title>
  <link href="http://www.wuwenjie.tk/blog/categories/data-mining/atom.xml" rel="self"/>
  <link href="http://www.wuwenjie.tk/"/>
  <updated>2015-10-05T17:18:06+08:00</updated>
  <id>http://www.wuwenjie.tk/</id>
  <author>
    <name><![CDATA[wuwenjie]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[转]Facebook加速deep-learning研发]]></title>
    <link href="http://www.wuwenjie.tk/blog/2015/08/31/zhuan-facebookjia-su-deep-learningyan-fa/"/>
    <updated>2015-08-31T20:16:00+08:00</updated>
    <id>http://www.wuwenjie.tk/blog/2015/08/31/zhuan-facebookjia-su-deep-learningyan-fa</id>
    <content type="html"><![CDATA[<h2>引言</h2>

<ul>
<li><a href="https://zh.wikipedia.org/wiki/%E5%A4%A7%E6%95%B8%E6%93%9A">大数据</a>时代的到来是无可质疑的，但挖掘数据背后的秘密才更具价值。</li>
<li>深度学习，受益于大数据的出现和大规模计算能力的提升，它是开启未来的钥匙。</li>
</ul>


<h2>文章简介</h2>

<ul>
<li>转载的文章是Keith Adams的一份第八届超大数据库会议（XLDB2015）报告。</li>
<li>报告简述了深度学习的所遇到的困难问题与解决方法。</li>
<li>转载文章自<a href="http://www.infoq.com/cn/news/2015/07/facebook-accelerate-deep-learnin">infoq</a>，作者张天雷，有删改。</li>
</ul>


<!-- more -->


<h2>正文</h2>

<h3>前言</h3>

<blockquote><p>近年来，人工智能领域所取得的许多进步都与“深度学习”息息相关。
深度学习的多层非线性结构使其具备强大的特征表达能力和对复杂任务的建模能力。
构建成熟的深度学习模型常常涉及到巨大的训练数据集，大的模型以及超长的计算时间，
因此，深层模型的并行化框架和训练加速方法是深度学习走向实用的重要基石。</p></blockquote>

<h3>随机<a href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95">梯度下降</a>（SGD）</h3>

<ul>
<li>深度学习的参数训练过程中，随机梯度下降（SGD）是最常被使用的方法。</li>
<li>在这个过程中，通常需要很多的技巧:

<ul>
<li>模型深度的选择</li>
<li>神经元个数的设定</li>
<li>训练权重的初始化</li>
<li>学习率的调整</li>
<li>Mini-batch的控制等等</li>
</ul>
</li>
<li>即便对这些技巧十分精通，实践中也要多次训练，反复摸索尝试。</li>
<li>此外，随机梯度下降本质上<strong>串行</strong>的.

<ul>
<li>深层模型参数多</li>
<li>计算量大</li>
<li>训练数据的规模更大</li>
<li>消耗很多计算资源</li>
<li>训练时间会非常长</li>
</ul>
</li>
<li>针对深度学习面临的这些挑战，Keith介绍了他们团队在加速深度学习方面所采用几种方法。</li>
</ul>


<h3>GPGPU加速</h3>

<ul>
<li>GPU（Graphic Process Units）的众核体系包含千个流处理器，可将运算并行化执行，大幅缩短模型的运算时间。</li>
<li>NVIDIA、AMD不断推进GPU大规模并行支持，General-Purposed GPU成为加速并行应用程序的重要手段。</li>
<li>GPU众核（many-core）体系结构，程序在GPU上相较于单核CPU提升几十倍乃至上千倍。</li>
<li>目前GPU已经发展到了较为成熟的阶段。用来训练深度神经网络，可充分发挥其数以千计核心的高效并行计算能力。</li>
<li>在使用海量训练数据的场景下，所耗费的时间大幅缩短，占用的服务器也更少。</li>
<li>如果针对适当的深度神经网络进行合理优化，一块GPU卡可相当于数十甚至上百台CPU服务器的计算能力。</li>
<li>GPU是深度学习模型训练方面的首选解决方案，也是Facebook当前加速深度学习的最重要方式。</li>
</ul>


<h3>数据并行（Data Parallel）</h3>

<ul>
<li>数据并行是指将训练数据切分为N份，分配给N个worker对N个分片的数据并行训练。</li>
<li>完成数据并行训练之后，模型的梯度是所有分片数据上<strong><a href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6">梯度</a></strong>的平均值，然后使用这个均值对参数进行更新。</li>
<li>再将更新的参数返回给N个worker进行下一次的迭代。</li>
<li>在训练的过程中，多个训练过程相互独立，每次迭代过程中worker之间的通信等比于参数的数量。</li>
<li>数据并行也存在自身的缺点，当训练数据太多时，数据并行不得不减小学习率，以保证训练过程的平稳。</li>
</ul>


<h3>模型并行（Model Parallel）</h3>

<ul>
<li>将模型拆分成几个单元，每一层都可以被拆分，由不同的训练单元分别持有，共同协作完成训练。</li>
<li>当一个计算单元上的神经元的输入来自另一个训练单元上的神经元的输出时，不同计算单元之间就产生通信开销。</li>
<li>拆分单元太多时，神经元输出值的通信量会急剧增加，导致模型的效率大幅下降。</li>
<li>多数情况下，模型并行带来的通信开销和同步消耗超过数据并行，因此加速比也不及数据并行。</li>
<li>但对于单机内存无法容纳的大模型来说，模型并行提供了一个很好的选择。</li>
<li>亦可组合数据并行和模型并行产生混合架构。</li>
</ul>


<h3>基于<a href="https://github.com/torch/torch7">Torch7</a>的深度学习（Productive Deep Learning with Torch7）</h3>

<ul>
<li>Torch7是一个为机器学习算法提供广泛支持的科学计算框架。</li>
<li>神经网络工具包实现了以下基础模块，可以方便地配置出目标多层神经网络。

<ul>
<li>均方标准差代价函数</li>
<li>非线性激活函数</li>
<li>梯度下降训练神经网络算法等</li>
</ul>
</li>
<li>Torch是机器学习和人工智能项目的核心。

<ul>
<li>不仅学术界，连google、Twitter和英特尔等企业也都使用这一架构。</li>
</ul>
</li>
<li>Facebook开发了一些能够在Torch7上更快速地训练神经网络的模块，加快了基于Torch的深度学习项目的运行速度。

<ul>
<li>允许开发者使用多个GPU进行参数的并行训练。</li>
<li>可使卷积神经网络的训练速度提升数十倍以上，而卷积神经网络是很多深度学习系统的核心。</li>
<li>另外，Facebook还推出了多款工具，速度常常比Torch默认工具快3至10倍。</li>
</ul>
</li>
</ul>


<h3>参数服务器架构（Parameter Server Architecture）</h3>

<ul>
<li>CPU集群方案的基本架构包含：

<ul>
<li>用于执行训练任务的Worker</li>
<li>用于分布式存储分发模型的参数服务器（Parameter Server）</li>
<li>用于协调整体任务的主控程序（Master）</li>
</ul>
</li>
<li>CPU集群方案适合训练GPU内存难以容纳的大模型，以及稀疏连接神经网络。</li>
<li>Keith还对Andrew Ng和Jeff Dean提出的参数服务器架构进行了简单介绍。

<ul>
<li>他们在Google用1000台CPU服务器，完成了模型并行和Downpour SGD数据并行的深度神经网络训练。</li>
<li>Andrew Ng和Jeff Dean的这项<a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks">成果</a>发表在NIPS2012上。</li>
</ul>
</li>
</ul>


<h3>总结</h3>

<ul>
<li>报告中，还提到了Tensor DSM等其他的方式用于加速深度学习。</li>
<li><blockquote><p>深度学习可通过学习一种深层非线性网络结构，实现复杂函数逼近，并展现了强大的学习数据集本质和高度抽象化特征的能力。</p></blockquote></li>
<li><blockquote><p>但是其面临的最大问题是如何解决其过长的计算时间，
只有强有力的基础设施和定制化的并行计算框架，才能让以往不可想象的训练任务加速完成，为深度学习走向实用奠定坚实的基础。</p></blockquote></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K-means聚类的应用]]></title>
    <link href="http://www.wuwenjie.tk/blog/2015/05/31/k-meansju-lei-de-ying-yong/"/>
    <updated>2015-05-31T19:02:00+08:00</updated>
    <id>http://www.wuwenjie.tk/blog/2015/05/31/k-meansju-lei-de-ying-yong</id>
    <content type="html"><![CDATA[<h1>K-means 简介</h1>

<ul>
<li>K-means算法（k-means clustering）：一种适用于大样本的无监督式的聚类分析方法。</li>
<li>我对算法基本原理的理解：</li>
<li>1.随机初始化k个聚类中心，也可以指定聚类中心。</li>
<li>2.计算样本与聚类中心的距离，将样本划分到最近的聚类中心的类里。</li>
<li>3.划分完毕后，计算每个类新的聚类中心，可以采用不同算法计算。</li>
<li>4.如果新的聚类中心没有变化，算法结束；如果有变化，goto 2、3。</li>
</ul>


<h1>K-means 应用</h1>

<ul>
<li>由于它流行于数据挖掘领域，常用来探索未知客群的结构。</li>
<li>在划分问题中，作为预处理工作，划分出了大致类别，然后可探究类内特性和差异。</li>
<li>其他：可以用作一种剔除算法、<a href="http://zh.wikipedia.org/zh/K-%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95">向量的量化、特征学习</a>。</li>
</ul>


<h1>K-means 的 R 实践</h1>

<ul>
<li>使用R语言使用K-means算法快捷方便。</li>
</ul>


<!-- more -->


<p>```js</p>

<h1>初始化数据</h1>

<p>m &lt;&ndash; rbind(
matrix(rnorm(100, sd = 0.6), ncol = 2), # 标准正态分布
matrix(rnorm(100, mean = 2, sd = 0.6), ncol = 2),
matrix(rnorm(100, mean = 4, sd = 0.6), ncol = 2),
matrix(rnorm(50, mean = -3, sd = 0.6), ncol = 2)
)
colnames(m) &lt;&ndash; c(&ldquo;x&rdquo;, &ldquo;y&rdquo;)</p>

<p>m  &lt;&ndash; apply(m,2,scale) # 标准化
cl &lt;&ndash; kmeans(m, 4) # 使用Kmeans进行聚类分析</p>

<p>cl</p>

<p>cl$size  # 聚类每个分组的数量
cl$totss # The total sum of squares.
cl$withinss # Vector of within-cluster sum of squares
cl$centers  # 聚类中心
1-(cl$tot.withinss/cl$totss)  #1-(sum(cl$withinss)/cl$totss)</p>

<h6>#</h6>

<h1>K-means 确定组数1</h1>

<p>png(&ldquo;Kmeans_group_1.png&rdquo;) # 输出图像到png文件</p>

<p>wss &lt;&ndash; (nrow(m)-1)*sum(apply(m,2,var))</p>

<p>for(i in 2:20)</p>

<pre><code>wss[i] &lt;- sum(kmeans(m,centers=i)$withinss)
</code></pre>

<p>plot(1:20,wss,type=&ldquo;b&rdquo;,xlab=&ldquo;No. Clusters&rdquo;,</p>

<pre><code>    ylab="Within groups sum of squares",
    main="Kmeans Centers Method 1")
</code></pre>

<p>identify(wss)
dev.off()</p>

<h1>K-means 确定组数2</h1>

<p>png(&ldquo;Kmeans_group_2.png&rdquo;)</p>

<p>wt &lt;&ndash; c()
for(i in 1:20){</p>

<pre><code>ks &lt;- kmeans(m,centers=i)
wt[i] &lt;- (1 - (ks$tot.withinss / ks$totss))
</code></pre>

<p>}
plot(1:20,wt,type=&ldquo;b&rdquo;,xlab=&ldquo;No. Clusters&rdquo;,</p>

<pre><code>    ylab="tot withinss / totss",
    main="Kmeans Centers Method 2")
</code></pre>

<p>abline(h=0.9);identify(wt)</p>

<p>dev.off()</p>

<h6>#</h6>

<h1>找出内部点</h1>

<p>resid.m &lt;&ndash; m &ndash; fitted(cl)</p>

<h1>计算 样本与对应中心点的差</h1>

<h1>cluster centers &ldquo;fitted&rdquo; to each obs.</h1>

<h2>计算距离</h2>

<p>distance &lt;&ndash; function(x){sqrt(x<a href="http://zh.wikipedia.org/zh/K-%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95">1</a>^2+x[2]^2)}
dis &lt;&ndash; apply(resid.m,1,distance)</p>

<h1>将距离与样本整合在一起</h1>

<p>m &lt;&ndash; as.data.frame(cbind(m,cl$cluster,dis))
colnames(m) &lt;&ndash; c(&ldquo;x&rdquo;, &ldquo;y&rdquo;,&ldquo;cluster&rdquo;,&ldquo;dis&rdquo;)</p>

<p>inpoint &lt;&ndash; c()</p>

<h1>筛选出每个类中内部的样本 小于1.2倍的类内平均距离</h1>

<p>for (i in 1:length(cl$size)){</p>

<pre><code># print(i)

tm &lt;- m[which(m$cluster == i),]

means &lt;- mean(tm$dis) #每群的平均距离

tpoint &lt;- tm[which(tm$dis &lt;= 1.2*means),]
# &lt;每群的平均距离,在类内部

inpoint &lt;- rbind(inpoint,tpoint)
</code></pre>

<p>}</p>

<p>inpoint &lt;&ndash; inpoint[,c(&ldquo;x&rdquo;,&ldquo;y&rdquo;)]
m &lt;&ndash; m[,c(&ldquo;x&rdquo;,&ldquo;y&rdquo;)]</p>

<p>png(&ldquo;Kmeans_inside.png&rdquo;)</p>

<h1>设置背景颜色</h1>

<p>par(bg = &ldquo;azure&rdquo;)</p>

<h1>画出聚类样本</h1>

<p>plot(m,col = cl$cluster,main=&ldquo;K均值聚类结果与类内聚集点&rdquo;)</p>

<h1>画出样本中心</h1>

<p>points(cl$centers, col = 1:length(cl$size), pch = 8, cex = 5)</p>

<h1>画出内部点</h1>

<p>points(inpoint, col = 1:nrow(inpoint), pch = 1, cex = 2)</p>

<p>dev.off()</p>

<p>```</p>

<ul>
<li>确定分类组数方法：</li>
<li><img src="/images/Kmeans_group_1.png" alt="确定组数方法1" /></li>
<li><img src="/images/Kmeans_group_2.png" alt="确定组数方法2" /></li>
</ul>


<h1>其他应用</h1>

<ul>
<li>最后找出内部点中，可以用作大样本快速SVM的样本筛选方法</li>
<li>因为支持向量只由超平面决定，而样本外部的点才能影响SV</li>
<li>所以可以将内部的点剔除而加快SVM的计算效率</li>
<li>如下图没有圈中的可作为训练样本，剔除可按需要进行</li>
<li><img src="/images/Kmeans_inside.png" alt="K均值聚类结果与类内聚集点" /></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[R语言简介]]></title>
    <link href="http://www.wuwenjie.tk/blog/2015/04/30/ryu-yan-jian-jie/"/>
    <updated>2015-04-30T18:14:00+08:00</updated>
    <id>http://www.wuwenjie.tk/blog/2015/04/30/ryu-yan-jian-jie</id>
    <content type="html"><![CDATA[<h2>R？</h2>

<ul>
<li>1993年<a href="http://www.r-project.org/">R</a>诞生了，而其前者<a href="http://zh.wikipedia.org/wiki/S%E8%AA%9E%E8%A8%80">S语言</a>早在1975年的贝尔实验室就被研发出来了。</li>
<li>R是<a href="https://directory.fsf.org/wiki/R">GNU计划的一个项目</a>，是S的开源实现，所以亦称为GNU s。</li>
<li>R是优秀的计算、绘图、统计分析系统，此外由用户撰写的包使得其功能更为强大。</li>
</ul>


<!-- more -->


<h2>正文</h2>

<ul>
<li></li>
</ul>


<iframe src ="http://www.wuwenjie.tk/images/R简介.pdf" width="800" height="1000">
<p>你的浏览器不支持iframes！</p>
</iframe>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[改]SAS-安装与简介]]></title>
    <link href="http://www.wuwenjie.tk/blog/2015/01/31/gai-sas-an-zhuang-yu-jian-jie/"/>
    <updated>2015-01-31T15:43:00+08:00</updated>
    <id>http://www.wuwenjie.tk/blog/2015/01/31/gai-sas-an-zhuang-yu-jian-jie</id>
    <content type="html"><![CDATA[<h2>前言</h2>

<ul>
<li>SAS 是<a href="http://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98">数据挖掘</a>界的巨无霸,几十年的发展业务遍布全球各行各业。</li>
<li>其在官网上介绍的<a href="http://www.sas.com/zh_cn/customers.html#--">客户案例</a>,有招行（包括Credit Card）、工商、交通、广发等等。</li>
<li>它是“邪恶的”商业软件，那我为什么还要用它呢？</li>
</ul>


<!-- more -->


<pre><code>* 使用简单，只要data步、proc步、options（选项）等即可，不需自己实现算法。
* 方法全，基本上囊括了大部分计量统计、数据挖掘方面的方法。
</code></pre>

<ul>
<li>问题：当你挖掘得深了以后，发现SAS会是你成长上的<a href="http://zh.wikipedia.org/zh/%E6%A1%8E%E6%A2%8F">桎梏</a>（zhì ɡù）。

<ul>
<li>因为算法的实现都被包裹起来了，你只能做参数的改动，无法改进算法。</li>
<li>SAS的简单语法也给做复杂项目，带来了很多麻烦。</li>
</ul>
</li>
<li>解决方案：以SAS作为入门级的工具 &ndash;> 进阶后可使用<a href="http://www.r-project.org/">R</a> &ndash;> 高级后使用编程语言。</li>
</ul>


<h2>下载</h2>

<ul>
<li>google搜索 ： SAS.9.2多国语言版.iso site:pan.baidu.com</li>
<li>约3GB，3,188,455,424字节。</li>
<li>用不了google搜索？ 可以使用<a href="https://github.com/lenbo-ma/gso">gso</a>,它是基于nodejs的开源google搜索代理服务。</li>
<li>可用列表 &ndash;> <a href="https://github.com/lenbo-ma/gso/wiki/%E5%8F%AF%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%88%97%E8%A1%A8">点这里</a></li>
</ul>


<h2>安装</h2>

<ul>
<li>下载后，解压SAS.9.2多国语言版.iso (md5sum:41b1b1098837d9a0cafb73fc8781055e)</li>
<li>搜索：SAS9.2多国语言版安装破解图解，<a href="http://wenku.baidu.com/view/f358faedba0d4a7302763ad4.html">破解教程</a>。</li>
<li>重要步骤：

<ul>
<li>将系统时间调整到2009年1月1日，适合于SID文件的时间，否则会被识别为过期。</li>
<li>替换sashost.dll文件，在你的安装路径\SASFoundation\9.2下。</li>
<li>破解完成后即可改回时间。</li>
</ul>
</li>
<li>破解文件：SID文件、sashost.dll (md5sum:a82df493b9faeb7bd261728345e413d1)</li>
<li>下载 &ndash;> <a href="/images/SAS9.2.zip">点这里</a></li>
</ul>


<h2>简介</h2>

<iframe src ="http://www.wuwenjie.tk/images/SAS教程.pdf" width="800" height="1000">
<p>你的浏览器不支持iframes！</p>
</iframe>



]]></content>
  </entry>
  
</feed>
